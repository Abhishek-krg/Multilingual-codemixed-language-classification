{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center;\">Offensive Language Classification</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import essential modules\n",
    "import pandas as pd\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll begin by loading and cleaning the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create df object to store dataset\n",
    "df_train=pd.read_csv('OffensiveLanguage/Task1/tamil_offensive_train.tsv', sep='\\t')\n",
    "df_test=pd.read_csv('OffensiveLanguage/Task1/tam_offesive_withoutlabels_test.tsv.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df_train.astype({'id':'string','text':'string','category':'string'}) #cast values to useable format\n",
    "df_test=df_test.astype({'id':'string','text':'string','category':'string'}) #cast values to useable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tam1</td>\n",
       "      <td>திருமலை நாயக்கர் பேரவை சார்பாக படம் வெற்றி பெற...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tam2</td>\n",
       "      <td>இந்த ட்ரெய்லர் கூட பார்க்கிற மாதிரி இல்லை.. இத...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tam3</td>\n",
       "      <td>மைசூரு செட்டியார் சமூகத்தின் சார்பாக இப்படம் வ...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tam4</td>\n",
       "      <td>மொத்த சாதியும் ஒரு சாதிக்கு எதிரா நிக்குது.......</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tam5</td>\n",
       "      <td>only for விஜய் சேதுபதி and STR</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text category\n",
       "0  tam1  திருமலை நாயக்கர் பேரவை சார்பாக படம் வெற்றி பெற...      NOT\n",
       "1  tam2  இந்த ட்ரெய்லர் கூட பார்க்கிற மாதிரி இல்லை.. இத...      OFF\n",
       "2  tam3  மைசூரு செட்டியார் சமூகத்தின் சார்பாக இப்படம் வ...      NOT\n",
       "3  tam4  மொத்த சாதியும் ஒரு சாதிக்கு எதிரா நிக்குது.......      OFF\n",
       "4  tam5                     only for விஜய் சேதுபதி and STR      NOT"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optional remove ... from end of comments\n",
    "for i in df.index:\n",
    "    df.loc[i,'text']=df.loc[i,'text'].strip('.')\n",
    "df.head() # notice the text in head() appears to have ... but thats just pandas max-display length limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tam1</td>\n",
       "      <td>திருமலை நாயக்கர் பேரவை சார்பாக படம் வெற்றி பெற...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tam2</td>\n",
       "      <td>இந்த ட்ரெய்லர் கூட பார்க்கிற மாதிரி இல்லை.. இத...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tam3</td>\n",
       "      <td>மைசூரு செட்டியார் சமூகத்தின் சார்பாக இப்படம் வ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tam4</td>\n",
       "      <td>மொத்த சாதியும் ஒரு சாதிக்கு எதிரா நிக்குது.......</td>\n",
       "      <td>OFF</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tam5</td>\n",
       "      <td>only for விஜய் சேதுபதி and STR</td>\n",
       "      <td>NOT</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text category label\n",
       "0  tam1  திருமலை நாயக்கர் பேரவை சார்பாக படம் வெற்றி பெற...      NOT     _\n",
       "1  tam2  இந்த ட்ரெய்லர் கூட பார்க்கிற மாதிரி இல்லை.. இத...      OFF     _\n",
       "2  tam3  மைசூரு செட்டியார் சமூகத்தின் சார்பாக இப்படம் வ...      NOT     _\n",
       "3  tam4  மொத்த சாதியும் ஒரு சாதிக்கு எதிரா நிக்குது.......      OFF     _\n",
       "4  tam5                     only for விஜய் சேதுபதி and STR      NOT     _"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add label coloumn in train df to convert catgories to machine usable labels\n",
    "df_train.insert(3,'label','_')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert category to machine usable labels\n",
    "for i in df_train.index:\n",
    "    if df_train.loc[i,'category']=='NOT':\n",
    "        df_train.loc[i,'label']=0\n",
    "    else:\n",
    "        df_train.loc[i,'label']=1\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate frequency dictionary and return max sentence length\n",
    "def gen_word_encodings(df,word_freq):\n",
    "    num_recs=df.index.stop\n",
    "    max_len=0\n",
    "    for i in df.index:\n",
    "        words=df['text'][i].strip().split()\n",
    "        if(len(words)>max_len):\n",
    "            max_len=len(words)\n",
    "        for word in words:\n",
    "            word_freq[word]+=1\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating Encodings\n",
    "word_freq=collections.Counter()\n",
    "max_len=gen_word_encodings(df_train,word_freq) #these 2steps are important to properly load the data \n",
    "#word to index maps as{\"word1\":idx1,\"word2\":idx2...}\n",
    "word2idx={x[0]:i+2 for i,x in enumerate(word_freq.most_common(len(word_freq)))}\n",
    "word2idx[\"PAD\"]=0\n",
    "word2idx[\"UNK\"]=1\n",
    "#idx to word mapping\n",
    "idx2word={v:k for k,v in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24754"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is not needed for dataset but we need this for stripping off the unwanted chars and emojis in dataset\n",
    "def gen_char_encodings(word_freq,char_freq):\n",
    "    max_word_len=0\n",
    "    for keys in word_freq:\n",
    "        if(len(keys)>max_word_len):\n",
    "            max_word_len=len(keys)\n",
    "        chars=list(keys)\n",
    "        for char in chars:\n",
    "            char_freq[char]+=1\n",
    "    return max_word_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_freq=collections.Counter()\n",
    "max_word_len=gen_char_encodings(word_freq,char_freq)\n",
    "char2idx={x[0]:i+2 for i,x in enumerate(char_freq.most_common(len(char_freq)))}\n",
    "char2idx[\"PAD\"]=0\n",
    "char2idx[\"UNK\"]=1\n",
    "#idx to char mapping\n",
    "idx2char={v:k for k,v in char2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_word_len # input length to embedding vector\n",
    "char2idx #('🤣', '🤔', '🦁', '🤦', '🤝', '🤗', '🤩', '🤪', '🥭', '🤫', '🤘', '🤬', '🤙', '🥳', '🤨', '🧐', '🥰', '🥇', '🥶', '🥊', '🤛', '🤞', '🤕', '🤭', '🤟', '🤐', '🤺', '🧡', '🦸', '⏮', '⏸', '⏭', '🤧', '\\U0001f7e0','🦍', '🧟', '🥁', '🤠', '🦌', '🦄', '🤓', '🧨', '🤮', '⏰', '🦅', '\\u2066', '\\u2069', '\\u200b', '\\u200d', '\\u200c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove emojis and unwanted characters from the dataframe\n",
    "for i in df_train.index:\n",
    "    df_train.loc[i,'text']=df_train.loc[i,'text'].translate({ord(j): None for j in ('🦂','🤣', '🤔', '🦁', '🤦', '🤝', '🤗', '🤩', '🤪', '🥭', '🤫', '🤘', '🤬', '🤙', '🥳', '🤨', '🧐', '🥰', '🥇', '🥶', '🥊', '🤛', '🤞', '🤕', '🤭', '🤟', '🤐', '🤺', '🧡', '🦸', '⏮', '⏸', '⏭', '🤧', '\\U0001f7e0','🦍', '🧟', '🥁', '🤠', '🦌', '🦄', '🤓', '🧨', '🤮', '⏰', '🦅', '\\u2066', '\\u2069', '\\u200b', '\\u200d', '\\u200c','\\U000fe4eb')})\n",
    "\n",
    "# remove emojis and unwanted characters from the dataframe\n",
    "for i in df_test.index:\n",
    "    df_test.loc[i,'text']=df_test.loc[i,'text'].translate({ord(j): None for j in ('🦂','🤣', '🤔', '🦁', '🤦', '🤝', '🤗', '🤩', '🤪', '🥭', '🤫', '🤘', '🤬', '🤙', '🥳', '🤨', '🧐', '🥰', '🥇', '🥶', '🥊', '🤛', '🤞', '🤕', '🤭', '🤟', '🤐', '🤺', '🧡', '🦸', '⏮', '⏸', '⏭', '🤧', '\\U0001f7e0','🦍', '🧟', '🥁', '🤠', '🦌', '🦄', '🤓', '🧨', '🤮', '⏰', '🦅', '\\u2066', '\\u2069', '\\u200b', '\\u200d', '\\u200c','\\U000fe4eb')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we can now create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate dataset\n",
    "def create_dataset(df,x,y):\n",
    "    for i in df.index:\n",
    "        words=df['text'][i].strip().split()\n",
    "        seqs=[]\n",
    "        for word in words:\n",
    "            if word in word2idx:\n",
    "                seqs.append(word2idx[word])\n",
    "            else:\n",
    "                seqs.append(word2idx[\"UNK\"])\n",
    "        x[i]=seqs\n",
    "        cat=df['category'][i]\n",
    "        if(cat=='OFF'):\n",
    "            y[i]=1\n",
    "        else:\n",
    "            y[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_recs=df_train.index.stop\n",
    "x=np.empty((num_recs,),dtype=list) # x holds sentence vectors \n",
    "y=np.zeros((num_recs,),dtype=\"uint8\") # y holds category 1 for Offensive 0 for Not-Offensive\n",
    "create_dataset(df_train,x,y)\n",
    "x=tf.keras.preprocessing.sequence.pad_sequences(x,maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenxt=int(x.shape[0]*0.7)\n",
    "xt=x[:lenxt] # x-train split with 70% data\n",
    "yt=y[:lenxt] # y-train split with 70% data\n",
    "xtt=x[lenxt:] # x-evaluation split with 30% data\n",
    "ytt=y[lenxt:] # y-evaluation split with 30% data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1 LSTM units 64 perf ~80%\n",
    "vocab_size=len(word2idx)\n",
    "model=tf.keras.models.Sequential([tf.keras.layers.Embedding(vocab_size,128,mask_zero=True,input_length=50),\n",
    "                                  tf.keras.layers.LSTM(64,dropout=0.25,recurrent_dropout=0.1,return_sequences=True,unroll=True)\n",
    "                                  tf.keras.layers.Dense(1,activation=\"sigmoid\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2 bidirectional LSTM with 32 units perf ~81%\n",
    "# Test 3 bidirectional LSTM with 64 units perf ~81% increasing number of units does not effect perf\n",
    "vocab_size=len(word2idx)\n",
    "model=tf.keras.models.Sequential([tf.keras.layers.Embedding(vocab_size,128,input_length=50),\n",
    "                                tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,dropout=0.2,recurrent_dropout=0.2),merge_mode='concat'),\n",
    "                                tf.keras.layers.Dense(1,activation=\"sigmoid\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4 seperate forward backward LSTM layers in bidirectional with 32 units each perf ~82.5%\n",
    "# Test 5 seperate forward backward LSTM layers in bidirectional with 16 units each perf ~80%\n",
    "# Test 6 seperate forward backward LSTM layers in bidirectional with 32 units each increasing dropout to 30% perf ~82%\n",
    "# Test 7 seperate forward backward LSTM layers in bidirectional with 32 units each decreasing dropout to 25% and 10% perf ~82.7%\n",
    "\n",
    "vocab_size=len(word2idx)\n",
    "forward_layer = tf.keras.layers.LSTM(32,dropout=0.25,recurrent_dropout=0.1,return_sequences=False)\n",
    "backward_layer = tf.keras.layers.LSTM(32,activation='relu',dropout=0.25,recurrent_dropout=0.1,return_sequences=False,go_backwards=True)\n",
    "model=tf.keras.models.Sequential([tf.keras.layers.Embedding(vocab_size,128,input_length=50),\n",
    "                                tf.keras.layers.Bidirectional(forward_layer, backward_layer=backward_layer,merge_mode='concat'),\n",
    "                                tf.keras.layers.Dense(1,activation=\"sigmoid\")])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "184/184 [==============================] - 30s 164ms/step - loss: 0.4436 - accuracy: 0.8054\n",
      "Epoch 2/50\n",
      "184/184 [==============================] - 29s 159ms/step - loss: 0.1986 - accuracy: 0.9277\n",
      "Epoch 3/50\n",
      "184/184 [==============================] - 29s 158ms/step - loss: 0.0493 - accuracy: 0.9862\n",
      "Epoch 4/50\n",
      "184/184 [==============================] - 29s 155ms/step - loss: 0.0205 - accuracy: 0.9954\n",
      "Epoch 5/50\n",
      "184/184 [==============================] - 29s 160ms/step - loss: 0.0118 - accuracy: 0.9974\n",
      "Epoch 6/50\n",
      "184/184 [==============================] - 30s 161ms/step - loss: 0.0106 - accuracy: 0.9971\n",
      "Epoch 7/50\n",
      "184/184 [==============================] - 30s 160ms/step - loss: 0.0072 - accuracy: 0.9980\n",
      "Epoch 8/50\n",
      "184/184 [==============================] - 29s 160ms/step - loss: 0.0048 - accuracy: 0.9986\n",
      "Epoch 9/50\n",
      "184/184 [==============================] - 30s 162ms/step - loss: 0.0035 - accuracy: 0.9983\n",
      "Epoch 10/50\n",
      "184/184 [==============================] - 30s 160ms/step - loss: 0.0026 - accuracy: 0.9985\n",
      "Epoch 11/50\n",
      "184/184 [==============================] - 29s 160ms/step - loss: 0.0025 - accuracy: 0.9985\n",
      "Epoch 12/50\n",
      "184/184 [==============================] - 29s 160ms/step - loss: 0.0022 - accuracy: 0.9990\n",
      "Epoch 13/50\n",
      "184/184 [==============================] - 29s 159ms/step - loss: 0.0020 - accuracy: 0.9990\n",
      "Epoch 14/50\n",
      "184/184 [==============================] - 30s 161ms/step - loss: 0.0018 - accuracy: 0.9986\n",
      "Epoch 15/50\n",
      "184/184 [==============================] - 30s 162ms/step - loss: 0.0038 - accuracy: 0.9981\n",
      "Epoch 16/50\n",
      "184/184 [==============================] - 29s 159ms/step - loss: 0.0037 - accuracy: 0.9976\n",
      "Epoch 17/50\n",
      "184/184 [==============================] - 30s 163ms/step - loss: 0.0034 - accuracy: 0.9976\n",
      "Epoch 18/50\n",
      "184/184 [==============================] - 29s 160ms/step - loss: 0.0020 - accuracy: 0.9985\n",
      "Epoch 19/50\n",
      "184/184 [==============================] - 30s 162ms/step - loss: 0.0018 - accuracy: 0.9988\n",
      "Epoch 20/50\n",
      "184/184 [==============================] - 29s 159ms/step - loss: 0.0017 - accuracy: 0.9986\n",
      "Epoch 21/50\n",
      "184/184 [==============================] - 30s 163ms/step - loss: 0.0017 - accuracy: 0.9986\n",
      "Epoch 22/50\n",
      "184/184 [==============================] - 29s 160ms/step - loss: 0.0017 - accuracy: 0.9990\n",
      "Epoch 23/50\n",
      "184/184 [==============================] - 29s 158ms/step - loss: 0.0017 - accuracy: 0.9988\n",
      "Epoch 24/50\n",
      "184/184 [==============================] - 29s 157ms/step - loss: 0.0017 - accuracy: 0.9986\n",
      "Epoch 25/50\n",
      "184/184 [==============================] - 30s 163ms/step - loss: 0.0017 - accuracy: 0.9985\n",
      "Epoch 26/50\n",
      "184/184 [==============================] - 30s 162ms/step - loss: 0.0016 - accuracy: 0.9986\n",
      "Epoch 27/50\n",
      "184/184 [==============================] - 29s 160ms/step - loss: 0.0016 - accuracy: 0.9986\n",
      "Epoch 28/50\n",
      "184/184 [==============================] - 29s 159ms/step - loss: 0.0016 - accuracy: 0.9988\n",
      "Epoch 29/50\n",
      "184/184 [==============================] - 30s 163ms/step - loss: 0.0016 - accuracy: 0.9986\n",
      "Epoch 30/50\n",
      "184/184 [==============================] - 29s 158ms/step - loss: 0.0016 - accuracy: 0.9986\n",
      "Epoch 31/50\n",
      "184/184 [==============================] - 30s 162ms/step - loss: 0.0016 - accuracy: 0.9985\n",
      "Epoch 32/50\n",
      "184/184 [==============================] - 29s 157ms/step - loss: 0.0016 - accuracy: 0.9990\n",
      "Epoch 33/50\n",
      "184/184 [==============================] - 29s 159ms/step - loss: 0.0016 - accuracy: 0.9985\n",
      "Epoch 34/50\n",
      "184/184 [==============================] - 29s 158ms/step - loss: 0.0016 - accuracy: 0.9986\n",
      "Epoch 35/50\n",
      "184/184 [==============================] - 30s 160ms/step - loss: 0.0016 - accuracy: 0.9985\n",
      "Epoch 36/50\n",
      "184/184 [==============================] - 29s 158ms/step - loss: 0.0016 - accuracy: 0.9988\n",
      "Epoch 37/50\n",
      "184/184 [==============================] - 29s 159ms/step - loss: 0.0016 - accuracy: 0.9986\n",
      "Epoch 38/50\n",
      "184/184 [==============================] - 29s 156ms/step - loss: 0.0016 - accuracy: 0.9990\n",
      "Epoch 39/50\n",
      "184/184 [==============================] - 29s 157ms/step - loss: 0.0016 - accuracy: 0.9990\n",
      "Epoch 40/50\n",
      "184/184 [==============================] - 29s 160ms/step - loss: 0.0016 - accuracy: 0.9988\n",
      "Epoch 41/50\n",
      "184/184 [==============================] - 30s 163ms/step - loss: 0.0016 - accuracy: 0.9985\n",
      "Epoch 42/50\n",
      "184/184 [==============================] - 29s 159ms/step - loss: 0.0016 - accuracy: 0.9988\n",
      "Epoch 43/50\n",
      "184/184 [==============================] - 29s 157ms/step - loss: 0.0016 - accuracy: 0.9985\n",
      "Epoch 44/50\n",
      "184/184 [==============================] - 29s 158ms/step - loss: 0.0016 - accuracy: 0.9990\n",
      "Epoch 45/50\n",
      "184/184 [==============================] - 30s 161ms/step - loss: 0.0016 - accuracy: 0.9988\n",
      "Epoch 46/50\n",
      "184/184 [==============================] - 29s 159ms/step - loss: 0.0016 - accuracy: 0.9991\n",
      "Epoch 47/50\n",
      "184/184 [==============================] - 29s 160ms/step - loss: 0.0016 - accuracy: 0.9986\n",
      "Epoch 48/50\n",
      "184/184 [==============================] - 29s 158ms/step - loss: 0.0016 - accuracy: 0.9986\n",
      "Epoch 49/50\n",
      "184/184 [==============================] - 29s 160ms/step - loss: 0.0016 - accuracy: 0.9983\n",
      "Epoch 50/50\n",
      "184/184 [==============================] - 30s 161ms/step - loss: 0.0016 - accuracy: 0.9990\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "hist=model.fit(x,y,batch_size=32,epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to hdf5 file\n",
    "model.save('task1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=tf.keras.models.load_model('task1.h5')\n",
    "model.output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_test.index:\n",
    "    words=df_test.loc[i,'text'].strip().split()\n",
    "    seqs=[]\n",
    "    for word in words:\n",
    "        if word in word2idx:\n",
    "            seqs.append(word2idx[word])\n",
    "        else:\n",
    "            seqs.append(word2idx[\"UNK\"])\n",
    "    seqs=[seqs]\n",
    "    seqs=tf.keras.preprocessing.sequence.pad_sequences(seqs,maxlen=50)\n",
    "    if(model(seqs)[0][0]>0.40):\n",
    "        preds=\"OFF\"\n",
    "    else:\n",
    "        preds=\"NOT\"\n",
    "    df_test.loc[i,'category']=preds\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save test predictions to file\n",
    "df_test.to_csv('OffensiveLanguage/Task1/task1_submission.tsv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot model AUC curve\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(211)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(hist.history[\"accuracy\"],color='b',label='Train')\n",
    "plt.plot(hist.history[\"val_accuracy\"],color='g',label='Validation')\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title('Loss')\n",
    "plt.plot(hist.history[\"loss\"],color='#FFA500',label='Train')\n",
    "plt.plot(hist.history[\"val_loss\"],color='g',label='Validation')\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"none\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
