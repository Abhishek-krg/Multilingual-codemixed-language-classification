{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Task2/Tamil_hasoc_tanglish_test_withlabels.tsv',sep='\\t',names=['id','text','category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TA_HL101</td>\n",
       "      <td>@Asha Apo neenga atha government ku theriya pa...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TA_HL102</td>\n",
       "      <td>@Bala sundar ayyo sorry...antha line ah explai...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TA_HL105</td>\n",
       "      <td>@kalimuthu ne ena lusa...yaaru edhu panaalum e...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TA_HL109</td>\n",
       "      <td>1st baby ku neat ah feed panunga plzz ipdi iru...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TA_HL113</td>\n",
       "      <td>2012 il vazhgiromnu iruku ithula, apdina?</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TA_HL118</td>\n",
       "      <td>30 varusa kadan. 25 age engayo idikuthe</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TA_HL124</td>\n",
       "      <td>a vanitha veliya po ethuku thirumpi vantha</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TA_HL125</td>\n",
       "      <td>√†¬Æ‚Ä¢√†¬Ø¬Å√†¬Æ¬¥√†¬Æ¬®√†¬Ø¬ç√†¬Æ¬§√†¬ØÀÜ ga taste ah saptanum nu ...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TA_HL127</td>\n",
       "      <td>Aaiiii Jolly Yellam onnah polam onnah polam oa...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TA_HL128</td>\n",
       "      <td>aaluku etha mathri pesarathu,thurumbi vanthu p...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text category\n",
       "0  TA_HL101  @Asha Apo neenga atha government ku theriya pa...      NOT\n",
       "1  TA_HL102  @Bala sundar ayyo sorry...antha line ah explai...      NOT\n",
       "2  TA_HL105  @kalimuthu ne ena lusa...yaaru edhu panaalum e...      NOT\n",
       "3  TA_HL109  1st baby ku neat ah feed panunga plzz ipdi iru...      NOT\n",
       "4  TA_HL113          2012 il vazhgiromnu iruku ithula, apdina?      NOT\n",
       "5  TA_HL118            30 varusa kadan. 25 age engayo idikuthe      NOT\n",
       "6  TA_HL124         a vanitha veliya po ethuku thirumpi vantha      NOT\n",
       "7  TA_HL125  √†¬Æ‚Ä¢√†¬Ø¬Å√†¬Æ¬¥√†¬Æ¬®√†¬Ø¬ç√†¬Æ¬§√†¬ØÀÜ ga taste ah saptanum nu ...      NOT\n",
       "8  TA_HL127  Aaiiii Jolly Yellam onnah polam onnah polam oa...      NOT\n",
       "9  TA_HL128  aaluku etha mathri pesarathu,thurumbi vanthu p...      NOT"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=pd.read_excel('OffensiveLanguage/Task2/Tamil-Codemixed_offensive_data_Training-Tweet-HL.xlsx',engine='openpyxl',names=['id','text','category'])\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4999 entries, 0 to 4998\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        4939 non-null   object\n",
      " 1   text      4939 non-null   object\n",
      " 2   category  4939 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 117.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df3=pd.concat([df,df2],axis=0,ignore_index=True) #merge to training sets data\n",
    "df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df3.astype({'id':'string','text':'string','category':'string'}) # convert to a usable datatype\n",
    "df3.dropna(axis=0,inplace=True) # drop null values\n",
    "df3.insert(3,'idx',pd.RangeIndex(stop=4939),True)\n",
    "df3.set_index('idx',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4939"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df3.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_word_encodings(df,word_freq):\n",
    "    num_recs=len(df.index)\n",
    "    max_len=0\n",
    "    for i in df.index:\n",
    "        words=df['text'][i].strip().split()\n",
    "        if(len(words)>max_len):\n",
    "            max_len=len(words)\n",
    "        for word in words:\n",
    "            word_freq[word]+=1\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq=collections.Counter()\n",
    "max_len=gen_word_encodings(df3,word_freq) #these 2steps are important to properly load the data \n",
    "#word to index maps as{\"word1\":idx1,\"word2\":idx2...}\n",
    "word2idx={x[0]:i+2 for i,x in enumerate(word_freq.most_common(len(word_freq)))}\n",
    "word2idx[\"PAD\"]=0\n",
    "word2idx[\"UNK\"]=1\n",
    "#idx to word mapping\n",
    "idx2word={v:k for k,v in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_char_encodings(word_freq,char_freq):\n",
    "    max_word_len=0\n",
    "    for keys in word_freq:\n",
    "        if(len(keys)>max_word_len):\n",
    "            max_word_len=len(keys)\n",
    "        chars=list(keys)\n",
    "        for char in chars:\n",
    "            char_freq[char]+=1\n",
    "    return max_word_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_freq=collections.Counter()\n",
    "max_word_len=gen_char_encodings(word_freq,char_freq)\n",
    "char2idx={x[0]:i+2 for i,x in enumerate(char_freq.most_common(len(char_freq)))}\n",
    "char2idx[\"PAD\"]=0\n",
    "char2idx[\"UNK\"]=1\n",
    "#idx to char mapping\n",
    "idx2char={v:k for k,v in char2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "char2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove emojis\n",
    "for i in df3.index:\n",
    "    df3.loc[i,'text']=df3.loc[i,'text'].translate({ord(j): None for j in ('ü§ó','üòû','üí™','üòÄ','\\xad','üò≠','üôå','\\x90','üòß','üôè','\\x81','‚Ñ¢','\\x8f','üò°','\\x8d','ü¶Ç','ü§£', 'ü§î', 'ü¶Å', 'ü§¶', 'ü§ù', 'ü§ó', 'ü§©', 'ü§™', 'ü•≠', 'ü§´', 'ü§ò', 'ü§¨', 'ü§ô', 'ü•≥', 'ü§®', 'üßê', 'ü•∞', 'ü•á', 'ü•∂', 'ü•ä', 'ü§õ', 'ü§û', 'ü§ï', 'ü§≠', 'ü§ü', 'ü§ê', 'ü§∫', 'üß°', 'ü¶∏', '‚èÆ', '‚è∏', '‚è≠', 'ü§ß', '\\U0001f7e0','ü¶ç', 'üßü', 'ü•Å', 'ü§†', 'ü¶å', 'ü¶Ñ', 'ü§ì', 'üß®', 'ü§Æ', '‚è∞', 'ü¶Ö', '\\u2066', '\\u2069', '\\u200b', '\\u200d', '\\u200c','\\U000fe4eb')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate dataset\n",
    "def create_dataset(df,x,y):\n",
    "    for i in df.index:\n",
    "        words=df['text'][i].strip().split()\n",
    "        seqs=[]\n",
    "        for word in words:\n",
    "            if word in word2idx:\n",
    "                seqs.append(word2idx[word])\n",
    "            else:\n",
    "                seqs.append(word2idx[\"UNK\"])\n",
    "        x[i]=seqs\n",
    "        cat=df['category'][i]\n",
    "        if(cat=='OFF'):\n",
    "            y[i]=1\n",
    "        else:\n",
    "            y[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_recs=len(df3.index)\n",
    "x=np.empty((num_recs,),dtype=list) # x holds sentence vectors \n",
    "y=np.zeros((num_recs,),dtype=\"uint8\") # y holds category 1 for Offensive 0 for Not-Offensive\n",
    "create_dataset(df3,x,y)\n",
    "x=tf.keras.preprocessing.sequence.pad_sequences(x,maxlen=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=len(word2idx)\n",
    "forward_layer = tf.keras.layers.LSTM(32,dropout=0.25,recurrent_dropout=0.1,return_sequences=False)\n",
    "backward_layer = tf.keras.layers.LSTM(32,activation='relu',dropout=0.25,recurrent_dropout=0.1,return_sequences=False,go_backwards=True)\n",
    "model=tf.keras.models.Sequential([tf.keras.layers.Embedding(vocab_size,128,input_length=128),\n",
    "                                tf.keras.layers.Bidirectional(forward_layer, backward_layer=backward_layer,merge_mode='concat'),\n",
    "                                tf.keras.layers.Dense(1,activation=\"sigmoid\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "155/155 [==============================] - 50s 321ms/step - loss: 0.6742 - accuracy: 0.5717\n",
      "Epoch 2/50\n",
      "155/155 [==============================] - 49s 316ms/step - loss: 0.2743 - accuracy: 0.9024\n",
      "Epoch 3/50\n",
      "155/155 [==============================] - 48s 312ms/step - loss: 0.0374 - accuracy: 0.9901\n",
      "Epoch 4/50\n",
      "155/155 [==============================] - 49s 315ms/step - loss: 0.0095 - accuracy: 0.9988\n",
      "Epoch 5/50\n",
      "155/155 [==============================] - 51s 328ms/step - loss: 0.0057 - accuracy: 0.9994\n",
      "Epoch 6/50\n",
      "155/155 [==============================] - 49s 315ms/step - loss: 0.0055 - accuracy: 0.9990\n",
      "Epoch 7/50\n",
      "155/155 [==============================] - 48s 313ms/step - loss: 0.0052 - accuracy: 0.9992\n",
      "Epoch 8/50\n",
      "155/155 [==============================] - 49s 314ms/step - loss: 0.0029 - accuracy: 0.9994\n",
      "Epoch 9/50\n",
      "155/155 [==============================] - 49s 315ms/step - loss: 0.0032 - accuracy: 0.9990\n",
      "Epoch 10/50\n",
      "155/155 [==============================] - 48s 312ms/step - loss: 0.0023 - accuracy: 0.9990\n",
      "Epoch 11/50\n",
      "155/155 [==============================] - 49s 314ms/step - loss: 0.0019 - accuracy: 0.9988\n",
      "Epoch 12/50\n",
      "155/155 [==============================] - 49s 315ms/step - loss: 0.0019 - accuracy: 0.9988\n",
      "Epoch 13/50\n",
      "155/155 [==============================] - 49s 314ms/step - loss: 0.0014 - accuracy: 0.9992\n",
      "Epoch 14/50\n",
      "155/155 [==============================] - 49s 316ms/step - loss: 0.0013 - accuracy: 0.9988\n",
      "Epoch 15/50\n",
      "155/155 [==============================] - 49s 316ms/step - loss: 0.0011 - accuracy: 0.9992\n",
      "Epoch 16/50\n",
      "155/155 [==============================] - 49s 316ms/step - loss: 0.0013 - accuracy: 0.9992\n",
      "Epoch 17/50\n",
      "155/155 [==============================] - 49s 314ms/step - loss: 0.0011 - accuracy: 0.9992\n",
      "Epoch 18/50\n",
      "155/155 [==============================] - 49s 313ms/step - loss: 0.0021 - accuracy: 0.9990\n",
      "Epoch 19/50\n",
      "155/155 [==============================] - 49s 317ms/step - loss: 0.0011 - accuracy: 0.9992\n",
      "Epoch 20/50\n",
      "155/155 [==============================] - 49s 315ms/step - loss: 0.0011 - accuracy: 0.9992\n",
      "Epoch 21/50\n",
      "155/155 [==============================] - 49s 317ms/step - loss: 0.0010 - accuracy: 0.9990\n",
      "Epoch 22/50\n",
      "155/155 [==============================] - 49s 315ms/step - loss: 9.3017e-04 - accuracy: 0.9994\n",
      "Epoch 23/50\n",
      "155/155 [==============================] - 49s 316ms/step - loss: 0.0010 - accuracy: 0.9990\n",
      "Epoch 24/50\n",
      "155/155 [==============================] - 49s 314ms/step - loss: 9.5512e-04 - accuracy: 0.9990\n",
      "Epoch 25/50\n",
      "155/155 [==============================] - 49s 314ms/step - loss: 0.0011 - accuracy: 0.9990\n",
      "Epoch 26/50\n",
      "155/155 [==============================] - 49s 317ms/step - loss: 0.0010 - accuracy: 0.9990\n",
      "Epoch 27/50\n",
      "155/155 [==============================] - 49s 316ms/step - loss: 9.4030e-04 - accuracy: 0.9992\n",
      "Epoch 28/50\n",
      "155/155 [==============================] - 49s 317ms/step - loss: 9.2017e-04 - accuracy: 0.9988\n",
      "Epoch 29/50\n",
      "155/155 [==============================] - 49s 314ms/step - loss: 9.4125e-04 - accuracy: 0.9994\n",
      "Epoch 30/50\n",
      "155/155 [==============================] - 49s 313ms/step - loss: 9.0358e-04 - accuracy: 0.9992\n",
      "Epoch 31/50\n",
      "155/155 [==============================] - 49s 317ms/step - loss: 8.7094e-04 - accuracy: 0.9994\n",
      "Epoch 32/50\n",
      "155/155 [==============================] - 49s 315ms/step - loss: 9.3306e-04 - accuracy: 0.9988\n",
      "Epoch 33/50\n",
      "155/155 [==============================] - 49s 316ms/step - loss: 8.9928e-04 - accuracy: 0.9992\n",
      "Epoch 34/50\n",
      "155/155 [==============================] - 49s 318ms/step - loss: 9.4599e-04 - accuracy: 0.9992\n",
      "Epoch 35/50\n",
      "155/155 [==============================] - 49s 314ms/step - loss: 8.9884e-04 - accuracy: 0.9994\n",
      "Epoch 36/50\n",
      "155/155 [==============================] - 49s 317ms/step - loss: 9.2804e-04 - accuracy: 0.9988\n",
      "Epoch 37/50\n",
      "155/155 [==============================] - 49s 314ms/step - loss: 8.5956e-04 - accuracy: 0.9996\n",
      "Epoch 38/50\n",
      "155/155 [==============================] - 49s 318ms/step - loss: 8.8289e-04 - accuracy: 0.9992\n",
      "Epoch 39/50\n",
      "155/155 [==============================] - 49s 318ms/step - loss: 9.0143e-04 - accuracy: 0.9988\n",
      "Epoch 40/50\n",
      "155/155 [==============================] - 49s 318ms/step - loss: 8.9740e-04 - accuracy: 0.9994\n",
      "Epoch 41/50\n",
      "155/155 [==============================] - 49s 314ms/step - loss: 9.0860e-04 - accuracy: 0.9990\n",
      "Epoch 42/50\n",
      "155/155 [==============================] - 49s 315ms/step - loss: 8.8663e-04 - accuracy: 0.9992\n",
      "Epoch 43/50\n",
      "155/155 [==============================] - 48s 309ms/step - loss: 8.6969e-04 - accuracy: 0.9992\n",
      "Epoch 44/50\n",
      "155/155 [==============================] - 49s 314ms/step - loss: 8.9815e-04 - accuracy: 0.9992\n",
      "Epoch 45/50\n",
      "155/155 [==============================] - 48s 310ms/step - loss: 8.6592e-04 - accuracy: 0.9994\n",
      "Epoch 46/50\n",
      "155/155 [==============================] - 48s 310ms/step - loss: 9.1913e-04 - accuracy: 0.9990\n",
      "Epoch 47/50\n",
      "155/155 [==============================] - 48s 313ms/step - loss: 8.6291e-04 - accuracy: 0.9994\n",
      "Epoch 48/50\n",
      "155/155 [==============================] - 48s 310ms/step - loss: 8.9550e-04 - accuracy: 0.9990\n",
      "Epoch 49/50\n",
      "155/155 [==============================] - 50s 323ms/step - loss: 8.6510e-04 - accuracy: 0.9990\n",
      "Epoch 50/50\n",
      "155/155 [==============================] - 49s 313ms/step - loss: 8.8572e-04 - accuracy: 0.9992\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "hist=model.fit(x,y,batch_size=32,epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('task2_mal.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.models.load_model('task2_tamil.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set for prediction\n",
    "df_preds=pd.read_csv('OffensiveLanguage/Task2/hasoc_tamil_task2_withoutlabels.tsv',sep='\\t',names=['id','text','category'])\n",
    "df_preds=df_preds.astype({'id':'string','text':'string','category':'string'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 999 entries, 0 to 998\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   ID      999 non-null    string\n",
      " 1   Tweets  999 non-null    string\n",
      "dtypes: string(2)\n",
      "memory usage: 15.7 KB\n"
     ]
    }
   ],
   "source": [
    "# load test set for prediction\n",
    "df_preds=pd.read_excel('Task2/mal-offensive-withoutlabels.xlsx',engine='openpyxl')\n",
    "df_preds=df_preds.astype({'ID':'string','Tweets':'string'})\n",
    "df_preds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'USER cheruparamadathil than thinnunnath alla pinarayi thinnunnath pinarayikk oru barber venam mudi kalayan jacob thomas vannal aa joli ayale elpikkum'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preds.loc[1,'Tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert category column to add predictions\n",
    "df_preds.insert(2,'category','NA',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_preds.index:\n",
    "    words=df_preds.loc[i,'text'].strip().split()\n",
    "    seqs=[]\n",
    "    for word in words:\n",
    "        if word in word2idx:\n",
    "            seqs.append(word2idx[word])\n",
    "        else:\n",
    "            seqs.append(word2idx[\"UNK\"])\n",
    "    seqs=[seqs]\n",
    "    seqs=tf.keras.preprocessing.sequence.pad_sequences(seqs,maxlen=50)\n",
    "    if(model(seqs)[0][0]>0.40):\n",
    "        preds=\"OFF\"\n",
    "    else:\n",
    "        preds=\"NOT\"\n",
    "    df_preds.loc[i,'category']=preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds.head()\n",
    "# save predictions\n",
    "df_preds.to_csv('OffensiveLanguage/Task2/task2_tamil.tsv',sep=\"\\t\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
